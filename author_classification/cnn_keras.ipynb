{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "12d10b1a-f6b2-4c27-b3c5-5746a9b40fee",
        "_uuid": "bd0110d9da8c5446c9aa50d97bc8c8d4c1424ff7"
      },
      "cell_type": "markdown",
      "source": "In this notebook we implement a convolutional neural network to classify the authors from some writtten text (https://arxiv.org/abs/1408.5882) on top of a pretrained GloVe embedding matrix (https://nlp.stanford.edu/projects/glove/)."
    },
    {
      "metadata": {
        "_cell_guid": "e5d6cdb9-0a56-44b2-99da-ae2432f89b9f",
        "_kg_hide-output": true,
        "_uuid": "eb67d93a8e3ae84d499f706568d7611627f5308a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras import regularizers, optimizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, LSTM, Flatten, Conv1D, MaxPooling1D, Dropout, Bidirectional, Concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n### Read in the data\n# read our data into a dataframe\ntexts = pd.read_csv(\"../input/spooky-author-identification/train.csv\")\n\ndef clean_str(string):\n    \"\"\"\n    From:\n    https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()\n\ntexts.text = texts.text.transform(clean_str)\n\nnum_classes = len(set(texts.author))\n# \none_hot_labels = np.zeros((len(texts), num_classes))\none_hot_labels[np.arange(one_hot_labels.shape[0]),texts.author.astype('category').cat.codes] = 1",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "49ce2a9b-0ba8-4edb-8b26-0d921db758e2",
        "collapsed": true,
        "_uuid": "f813a14b9f135e8f44eee3f70bcbb0978295a586",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load the pretrained GloVec embedding matrix\nembedding_size = 100\n\npretrained_embedding = {}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split(' ')\n        pretrained_embedding[values[0]] = np.asarray(values[1:], dtype='float32')\n        \ndef find_closest_word(word, vocab, pretrained_embedding):\n    if word not in pretrained_embedding: return 'UNK'\n    items = [(k,v) for k,v in pretrained_embedding.items() if k!=word and k in vocab]\n    idx = np.argmin([np.dot(v-pretrained_embedding[word], v-pretrained_embedding[word]) for k,v in items])\n    return items[idx][0]        ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "67947935-8ad7-4ec6-a72c-8f37c2caa087",
        "_uuid": "db1a2ed668cf006f0e4fbc0088a994ffb3ff6ec8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(texts.text, one_hot_labels, random_state=20171030, train_size=0.8)\n\n# Verify that test and train are well stratified\nprint(np.mean(y_train, axis = 0))\nprint(np.mean(y_test, axis = 0))",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[ 0.40247718  0.28998276  0.30754006]\n[ 0.40755873  0.27911134  0.31332993]\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n  FutureWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "66a8718b-fd61-4234-893f-547c57b7a85d",
        "collapsed": true,
        "_uuid": "51646e36877f1f4033cf55b810c79fe2b56391a9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "countvect = CountVectorizer(max_features=1000000, strip_accents='unicode').fit(X_train)\n\nif 'UNK' not in countvect.vocabulary_:\n    countvect.vocabulary_['UNK'] = max(countvect.vocabulary_.values()) + 1\n# Special\nfor c in ',ia':\n    if c not in countvect.vocabulary_: \n        countvect.vocabulary_[c] = max(countvect.vocabulary_.values()) + 1",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8465167c-c541-4b1e-a01c-25f0f6a6c064",
        "collapsed": true,
        "_uuid": "8ae9d7daf1c26a9d4b5e14f5531ea6526531004d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# transform input strings to list of index, padded to the same length\ndef text2idx(s, vocab, maxlen):\n    words = s.split(' ')\n    for i,w in enumerate(words): \n        if w not in vocab: \n            #words[i] = find_closest_word(w, vocab, pretrained_embedding)\n            words[i] = 'UNK'\n    return np.array([vocab[x] for x in words] + [vocab['UNK']]*(maxlen - len(words)))\n\npadded_len = max(X_train.str.len().max(), X_test.str.len().max())\nX_train_keras = X_train.apply(text2idx, vocab=countvect.vocabulary_, maxlen=padded_len)\nX_train_keras = np.array([x for x in X_train_keras])\n\nX_test_keras = X_test.apply(text2idx, vocab=countvect.vocabulary_, maxlen=padded_len)\nX_test_keras = np.array([x for x in X_test_keras])",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b295d519-9ab6-4565-86ec-e6a707280522",
        "_uuid": "474ca279cd93f607ab1c1fa91cb2e3cc7278e681",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Build the embedding matrix. Any word not found in the pretrained will be represented by all zero vector\nembedding_matrix = np.zeros((len(countvect.vocabulary_), embedding_size))\nfor w, i in countvect.vocabulary_.items():\n    embedding_vector = pretrained_embedding.get(w)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    #else:\n    #    print(w) # show the words not in pretrained",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b608487c-f4cd-49f7-9666-4ca0b43639ea",
        "scrolled": true,
        "_uuid": "6856a34e35cead0697357faecab7131f99f52b22",
        "trusted": true
      },
      "cell_type": "code",
      "source": "embedding_length = 100\nfilter_sizes = [3,4,5,6]\n\nmodel_input = Input(shape=(padded_len,), dtype='int32', name='embed_input') \nx = Embedding(len(countvect.vocabulary_), embedding_length, weights=[embedding_matrix], trainable=False)(model_input)\n\nconvs = []\nfor filter_size in filter_sizes: \n    conv = Conv1D(8, filter_size, padding=\"valid\", activation=\"relu\")(x) # sharing the input x\n    conv = MaxPooling1D(4)(conv)\n    conv = Flatten()(conv)\n    convs.append(conv)\n\n# merge the branches\nx = Concatenate()(convs)\nx = Dropout(0.5)(x)\nmodel_output = Dense(num_classes, activation='softmax')(x)\nmodel = Model(model_input, model_output)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n\nprint(model.summary())\nmodel.fit(X_train_keras, y_train, validation_data=(X_test_keras,  y_test), epochs=10, batch_size=128) ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\nembed_input (InputLayer)         (None, 4670)          0                                            \n____________________________________________________________________________________________________\nembedding_1 (Embedding)          (None, 4670, 100)     2300000     embed_input[0][0]                \n____________________________________________________________________________________________________\nconv1d_1 (Conv1D)                (None, 4668, 8)       2408        embedding_1[0][0]                \n____________________________________________________________________________________________________\nconv1d_2 (Conv1D)                (None, 4667, 8)       3208        embedding_1[0][0]                \n____________________________________________________________________________________________________\nconv1d_3 (Conv1D)                (None, 4666, 8)       4008        embedding_1[0][0]                \n____________________________________________________________________________________________________\nconv1d_4 (Conv1D)                (None, 4665, 8)       4808        embedding_1[0][0]                \n____________________________________________________________________________________________________\nmax_pooling1d_1 (MaxPooling1D)   (None, 1167, 8)       0           conv1d_1[0][0]                   \n____________________________________________________________________________________________________\nmax_pooling1d_2 (MaxPooling1D)   (None, 1166, 8)       0           conv1d_2[0][0]                   \n____________________________________________________________________________________________________\nmax_pooling1d_3 (MaxPooling1D)   (None, 1166, 8)       0           conv1d_3[0][0]                   \n____________________________________________________________________________________________________\nmax_pooling1d_4 (MaxPooling1D)   (None, 1166, 8)       0           conv1d_4[0][0]                   \n____________________________________________________________________________________________________\nflatten_1 (Flatten)              (None, 9336)          0           max_pooling1d_1[0][0]            \n____________________________________________________________________________________________________\nflatten_2 (Flatten)              (None, 9328)          0           max_pooling1d_2[0][0]            \n____________________________________________________________________________________________________\nflatten_3 (Flatten)              (None, 9328)          0           max_pooling1d_3[0][0]            \n____________________________________________________________________________________________________\nflatten_4 (Flatten)              (None, 9328)          0           max_pooling1d_4[0][0]            \n____________________________________________________________________________________________________\nconcatenate_1 (Concatenate)      (None, 37320)         0           flatten_1[0][0]                  \n                                                                   flatten_2[0][0]                  \n                                                                   flatten_3[0][0]                  \n                                                                   flatten_4[0][0]                  \n____________________________________________________________________________________________________\ndropout_1 (Dropout)              (None, 37320)         0           concatenate_1[0][0]              \n____________________________________________________________________________________________________\ndense_1 (Dense)                  (None, 3)             111963      dropout_1[0][0]                  \n====================================================================================================\nTotal params: 2,426,395\nTrainable params: 126,395\nNon-trainable params: 2,300,000\n____________________________________________________________________________________________________\nNone\nTrain on 15663 samples, validate on 3916 samples\nEpoch 1/10\n15663/15663 [==============================] - 610s - loss: 1.0050 - categorical_accuracy: 0.4947 - val_loss: 0.8715 - val_categorical_accuracy: 0.6226\nEpoch 2/10\n15663/15663 [==============================] - 608s - loss: 0.8374 - categorical_accuracy: 0.6232 - val_loss: 0.7780 - val_categorical_accuracy: 0.6647\nEpoch 3/10\n15663/15663 [==============================] - 617s - loss: 0.7748 - categorical_accuracy: 0.6586 - val_loss: 0.7462 - val_categorical_accuracy: 0.6775\nEpoch 4/10\n15663/15663 [==============================] - 644s - loss: 0.7471 - categorical_accuracy: 0.6729 - val_loss: 0.7493 - val_categorical_accuracy: 0.6780\nEpoch 5/10\n15663/15663 [==============================] - 647s - loss: 0.7143 - categorical_accuracy: 0.6922 - val_loss: 0.7178 - val_categorical_accuracy: 0.6999\nEpoch 6/10\n15663/15663 [==============================] - 642s - loss: 0.7028 - categorical_accuracy: 0.6990 - val_loss: 0.7067 - val_categorical_accuracy: 0.7061\nEpoch 7/10\n15663/15663 [==============================] - 754s - loss: 0.6849 - categorical_accuracy: 0.7026 - val_loss: 0.6998 - val_categorical_accuracy: 0.7074\nEpoch 8/10\n15663/15663 [==============================] - 847s - loss: 0.6703 - categorical_accuracy: 0.7159 - val_loss: 0.7042 - val_categorical_accuracy: 0.7045\nEpoch 9/10\n15663/15663 [==============================] - 658s - loss: 0.6611 - categorical_accuracy: 0.7185 - val_loss: 0.7028 - val_categorical_accuracy: 0.7035\nEpoch 10/10\n15663/15663 [==============================] - 652s - loss: 0.6413 - categorical_accuracy: 0.7305 - val_loss: 0.6833 - val_categorical_accuracy: 0.7176\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7efdfc450668>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "1c1bff5f-270f-4d1a-a909-2a274a7b626e",
        "collapsed": true,
        "_uuid": "6cf87ae97afdb6ab0545c6376afcebfa8573b5fa",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}